if(M1[i,1]<0.66 & M1[i,1]>=0.33)
{
val<- 0.5
}
if(M1[i,1]<=1 & M1[i,1]>=0.66)
{
val<- 1
}
if(val == M2[i,3])
{
points <- points + 1
}
#print("-------------------")
#message("Result ",M1[i,1])
#message("Scaled ",val)
#message("Ideal ",M2[i,3])
}
result <- ((points*100)/n)
result
}
PrintResult <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
print("----------RESULT------------")
message("Iterations: ",iters)
message("Learning factor: ",learn)
message("Neurons: ",neurons)
print("----------------------------")
}
Fitness  <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
thresh <- BinToDec(paste(X[21:24] , collapse =""))/100
result <- 0
tryCatch(
{
resultData <- CalculateMLP(iters,learn,neurons,thresh)
result <- Error(resultData,testSet)
print("Result")
print(result)
result
},warning = function(w){
result
},erro = function(e){
result
}
)
}
PrintResult(GA@solution)
#MLP arguments:
#1.Learning coefficient
#2.Momentum
#3.Bias
#4.Epoch number
#5.Hiden layer neauron number
setwd(dirname(getActiveDocumentContext()$path))
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(101)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#trainMatrix <- data.matrix(trainSet)
#testMatrix <- data.matrix(testSet)
CalculateMLP <- function(iters,learn,neurons,thresholdParam)
{
formula <- as.formula('V3 ~ V1 + V2')
nn <- neuralnet(formula,trainSet,neurons,threshold = thresholdParam, stepmax = iters,linear.output=FALSE, learningrate = learn)
prediction <- compute(nn,testSet[,1:2])
resultData<-prediction$net.result
resultData
}
source("MLP.R")
BinToDec <- function(x) {
sum(2^(which(rev(unlist(strsplit(as.character(x), "")) == 1))-1))
}
Error <- function(M1,M2)
{
points <- 0
n<-nrow(M1)
for(i in 1:n)
{
val <- 0
if(M1[i,1]<0.33)
{
val<- 0
}
if(M1[i,1]<0.66 & M1[i,1]>=0.33)
{
val<- 0.5
}
if(M1[i,1]<=1 & M1[i,1]>=0.66)
{
val<- 1
}
if(val == M2[i,3])
{
points <- points + 1
}
#print("-------------------")
#message("Result ",M1[i,1])
#message("Scaled ",val)
#message("Ideal ",M2[i,3])
}
result <- ((points*100)/n)
result
}
PrintResult <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
print("----------RESULT------------")
message("Iterations: ",iters)
message("Learning factor: ",learn)
message("Neurons: ",neurons)
print("----------------------------")
}
Fitness  <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
thresh <- BinToDec(paste(X[21:24] , collapse =""))/100
result <- 0
tryCatch(
{
resultData <- CalculateMLP(iters,learn,neurons,thresh)
result <- Error(resultData,testSet)
print("Result")
print(result)
result
},warning = function(w){
result
},erro = function(e){
result
}
)
}
source("MLP.R")
source("Main.R)
clean
source("MLP.R")
run("Main.R")
Rscript Main.R
Main.R
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(101)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
PrintResult(GA@solution)
GA@solution
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(101)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
#set.seed(101)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
#set.seed(101)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(103)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(104)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(105)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
source("MLP.R")
BinToDec <- function(x) {
sum(2^(which(rev(unlist(strsplit(as.character(x), "")) == 1))-1))
}
Error <- function(M1,M2)
{
points <- 0
n<-nrow(M1)
for(i in 1:n)
{
val <- 0
if(M1[i,1]<0.33)
{
val<- 0
}
if(M1[i,1]<0.66 & M1[i,1]>=0.33)
{
val<- 0.5
}
if(M1[i,1]<=1 & M1[i,1]>=0.66)
{
val<- 1
}
if(val == M2[i,3])
{
points <- points + 1
}
#print("-------------------")
#message("Result ",M1[i,1])
#message("Scaled ",val)
#message("Ideal ",M2[i,3])
}
result <- ((points*100)/n)
result
}
PrintResult <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
thresh <- BinToDec(paste(X[21:24] , collapse =""))/100
print("----------RESULT------------")
message("Iterations: ",iters)
message("Learning factor: ",learn)
message("Neurons: ",neurons)
message("Threshold: ",thresh)
print("----------------------------")
}
Fitness  <- function(X)
{
iters <- BinToDec(paste(X[1:10] , collapse =""))*20
learn <- (BinToDec(paste(X[11:15] , collapse ="")))/100
neurons <- BinToDec(paste(X[16:20] , collapse =""))
thresh <- BinToDec(paste(X[21:24] , collapse =""))/100
result <- 0
tryCatch(
{
resultData <- CalculateMLP(iters,learn,neurons,thresh)
result <- Error(resultData,testSet)
print("Result")
print(result)
result
},warning = function(w){
result
},erro = function(e){
result
}
)
}
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(106)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(107)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
library(iterators)
library(foreach)
library(GA)
library(rstudioapi)
library(neuralnet)
library(doParallel)
#set working directory
setwd(dirname(getActiveDocumentContext()$path))
#load files
source("HelperFunctions.R")
source("MLP.R")
####################
#     Prepare data
####################
wholeData = read.csv("datatrain.csv", header = FALSE,sep=",")
maxs <- apply(wholeData[,1:3], 2, max)
mins <- apply(wholeData[,1:3], 2, min)
scaledData <- scale(wholeData[,1:3],center = mins, scale = maxs - mins)
#set seed so both sets will be reproducable
set.seed(117)
sample <- sample.int(n = nrow(scaledData), size = floor(.50*nrow(scaledData)), replace = F)
trainSet <- scaledData[sample,]
testSet <- scaledData[-sample,]
#run genetic algorithm
GA <- ga("binary", fitness = Fitness, nBits = 24, maxiter = 20, popSize = 15, keepBest = TRUE)
summary(GA)
plot(GA)
GA@solution
PrintResult(GA@solution)
